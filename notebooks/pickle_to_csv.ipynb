{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2018 Esref Ozdemir\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pkl to csv Conversion\n",
    "In this notebook we convert **.pkl** files containing raw and event data to csv files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import pickle\n",
    "from os import listdir\n",
    "from os.path import join\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_raw_dir = '../data/pickle/processed'\n",
    "pickle_event_dir = '../data/pickle/event'\n",
    "csv_raw_dir = '../data/processed'\n",
    "csv_event_dir = '../data/event'\n",
    "\n",
    "event_cols = [\n",
    "    'teamId',\n",
    "    'eventId',\n",
    "    'jersey',\n",
    "    'half',\n",
    "    'minute',\n",
    "    'second',\n",
    "    'location',\n",
    "    'bodyPart',\n",
    "    'postLocation',\n",
    "    'custom'\n",
    "]\n",
    "\n",
    "raw_cols = [\n",
    "    'half',\n",
    "    'minute',\n",
    "    'second',\n",
    "    'isHomeTeam',\n",
    "    'teamId',\n",
    "    'jerseyNumber',\n",
    "    'x',\n",
    "    'y',\n",
    "    'distance',\n",
    "    'speed',\n",
    "    'hasballTeam',\n",
    "    'teamPoss',\n",
    "    'jerseyPoss',\n",
    "    'teamType'\n",
    "]\n",
    "\n",
    "event_pkl_regex = re.compile(r'\\d+_event.pkl')\n",
    "raw_pkl_regex = re.compile(r'\\d+_raw.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_csv(data, cols, out):\n",
    "    '''\n",
    "    Write a given list of tuples to a csv file.\n",
    "    \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data: A list of tuples containing the values for the specified columns.\n",
    "    cols: A list of strings containing the column names.\n",
    "    out: Output csv file.\n",
    "    '''\n",
    "    with open(out, 'w') as f:\n",
    "        csv_out = csv.writer(f)\n",
    "        csv_out.writerow(cols)\n",
    "        for row in data:\n",
    "            csv_out.writerow(row)\n",
    "            \n",
    "            \n",
    "def convert_event(pkl_filename):\n",
    "    with open(join(pickle_event_dir, pkl_filename), 'rb') as f:\n",
    "        event_data = pickle.load(f)\n",
    "    csv_filename = pkl_filename.split('.')[0] + '.csv'\n",
    "    write_csv(event_data, event_cols, join(csv_event_dir, csv_filename))\n",
    "    \n",
    "    \n",
    "def convert_raw(pkl_filename):\n",
    "    with open(join(pickle_raw_dir, pkl_filename), 'rb') as f:\n",
    "        raw_data = pickle.load(f)\n",
    "    accum_list = []\n",
    "    for key, val in raw_data.items():\n",
    "        key_tuple = tuple((int(num) for num in key.split('-')))\n",
    "        for pos_data in val:\n",
    "            accum_list.append(key_tuple + pos_data)\n",
    "    accum_list.sort(key=lambda x: x[:3])  # sort lexicographically with respect to half, minute, second.\n",
    "    csv_filename = pkl_filename.split('.')[0] + '.csv'\n",
    "    write_csv(accum_list, raw_cols, join(csv_raw_dir, csv_filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Event Conversion\n",
    "\n",
    "### Event .pkl Format\n",
    "An event **.pkl** file contains a list of tuples. Each tuple has the following format:\n",
    "\n",
    "```tuple = (teamId, eventId, jersey, half, minute, second, location, bodyPart, postLocation, custom)```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "event_pkl_files = (f for f in listdir(pickle_event_dir) if event_pkl_regex.match(f))\n",
    "\n",
    "pool = multiprocessing.Pool()\n",
    "pool.map(convert_event, event_pkl_files);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Raw Data Conversion\n",
    "\n",
    "### Raw .pkl Format\n",
    "A rawdata **.pkl** file contains a dictionary with\n",
    "* **key:** ```'%d-%d-%d'.format(half, minute, second)```\n",
    "* **val:** A list of tuples. Each tuple is\n",
    "  * ```tuple = (isHomeTeam, teamId, jerseyNumber, x, y, distance, speed, hasballteam, teamPoss, jerseyPoss, teamType)```\n",
    "    * **hasballTeam = teamId**\n",
    "    * **teamPoss = 1** if home team has possession.\n",
    "    * **teamType**\n",
    "      1. home team player\n",
    "      2. away team player\n",
    "      3. referee\n",
    "      4. home goalkeeper\n",
    "      5. away goalkeeper\n",
    "      \n",
    "We first convert the dictionary structure to a single list of tuples and then write the data to corresponding csv files.\n",
    "* Keys are written as three separate columns.\n",
    "* Each list is written sequentially.\n",
    "* Rows are written in increasing half-minute-second order for easier data processing, later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_pkl_files = (f for f in listdir(pickle_raw_dir) if raw_pkl_regex.match(f))\n",
    "\n",
    "pool = multiprocessing.Pool()\n",
    "pool.map(convert_raw, raw_pkl_files);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
